\epigraph{\textit{Logic is the foundation of the certainty of all the Knowledge we acquire.}}{-- \textup{Leonhard Euler}}

The main goal of this thesis is to create a tool capable of creating subsets out of a Knowledge graph. In this chapter, we will explore the possibility of implementing the Pregel framework in Rust, as well as a novel approach to the problem of Knowledge graph validation.

\section{Pregel-rs}



\section{PSchema-rs}

\paragraph{Parallelization}

The first optimization that we are going to consider is parallelization. The idea is to split the DuckDB dump into several chunks and then parse each of the chunks in parallel. This is going to be done by the first component of the tool. Then, we are going to parse each of the chunks in parallel. The idea is to use \texttt{rayon}\footnote{\url{https://github.com/rayon-rs/rayon}}. This library is a data-parallelism API for Rust that provides several parallel iterators. Hence, we are using it to create a \texttt{par\_lines} iterator for parsing the DuckDB dump in parallel. This can be done because each of the lines of the chunks of the dump is independent of each other. Thus, we can parse each of the entities in parallel without having to worry about synchronization issues. What's more, the \texttt{rayon} library is going to take care of the load balancing for us. This means that we do not have to worry about splitting the DuckDB dump into chunks of equal size. Lastly, \texttt{rayon} is also in charge of handling the thread pool for us and caring about the data-races that might arise.

What I like the most about \texttt{rayon} is the ease of transforming a sequential iterator into a parallel one. Let's have a look at an example of how this can be done:

\begin{minted}{rust}
    let lines = BufReader::new(file).lines();
    let lines = lines.into_par_iter();
\end{minted}

Whereas the first line creates a sequential iterator, the second one transforms it into a parallel one. This is done by calling the \texttt{into\_par\_iter} method on the iterator. Even if we have just shown a simple example, the actual solution is not far from what we have just shown above. See the code in the repository\footnote{\url{https://github.com/angelip2303/pschema-rs/blob/e51a24c194184db5f71bacb5289b8af55caa5146/src/backends/duckdb.rs\#L64}} for a more detailed example:

\begin{minted}{rust}
    let batches: Vec<RecordBatch> = match statement.query_arrow([]) {
        Ok(arrow) => arrow.collect(),
        Err(_) => return Err(String::from("Error executing the Arrow query")),
    };

    batches
        .into_par_iter()
        .map(|batch| {
            // Processing goes here...
        });
\end{minted}

In this case, we are allowed to do so as we are using Apache Arrow under the hood. Thus, we are handling a vector of Chunks that are