\epigraph{\textit{80\% of a piece of software can be written in 20\% of the total allocated time. Conversely, the hardest 20\% of the code takes 80\% of the time.}}{-- \textup{Roger S. Pressman}}

\section{\texttt{wd2duckdb}}

As we have seen in the previous chapter, the \texttt{wd2duckdb} tool is analyzed thoroughly for us to understand how it performs and what can be done to improve it. In this section, we will present the execution times of the tool provided several configurations. What we want to see is how the tool performs with different optimizations enabled and how it compares to the original version of the tool. As such, we will prove that the optimizations we have implemented are indeed useful and that they improve the performance of the tool. Recall that the optimizations that we have implemented were described in section \ref{section:optimizations}.

As can be seen in Figure \ref{fig:wd2duckdb}, the optimizations that we have implemented improve the performance of the tool. This is something that was expected before running the tests; however, the results obtained prove to be a worthy improvement. Note that for us to test the performance of the tool, we have used the same Wikidata dump for all the tests. The idea is to create 8 different databases each three times larger than the previous one. This, we have increased the number of Wikidata entities by a factor of 3 each time, starting from 10 thousand items to 21.87 million elements. This is, values in the abscissa axis
can be obtained through the following representation: $10,000 \cdot 3^n, n \in [0, 7]$. For us to fit such a big range of values in a single graph, we have used a logarithmic scale for the x-axis. This way, we can see the results of the tests in a single graph. Note that for the y-axis, we have used a linear scale, as the values are not that big, ranging from $0$ to $4,000$ and $40,000$ seconds, respectively, modeling the time it took to generate the database. This is said to be a \textit{semi-logarithmic} plot. To put it all together, we have used this type of representation to manage to test the tool with a wide range of values, and still, be able to see the results in a single graph.

For us to understand the values better, let me show them in a table, and then, we will comment on them in detail. Table \ref{table:wd2duckdb} shows the time it took to create the database with the different optimizations enabled and disabled.

\begin{figure}[p]
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includestandalone[width=\textwidth,height=8cm]{diagrams/13-1_wd2duckdbOPT}
        \caption{Having all the optimizations enabled}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includestandalone[width=\textwidth,height=8cm]{diagrams/13-2_wd2duckdbDEV}
        \caption{Having no optimization enabled}
    \end{subfigure}%
    \vspace*{1em}
    \begin{subfigure}{\textwidth}
        \centering
        \includestandalone[width=\textwidth]{diagrams/13-3_wd2duckdbBAR}
        \caption{Comparison between the two options: with and without optimizations}
    \end{subfigure}
    \caption{Time to create the database with \texttt{wd2duckdb}}
    \label{fig:wd2duckdb}
\end{figure}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        \rowcolor[HTML]{EFEFEF}
        \cellcolor[HTML]{C0C0C0}\textbf{N}               & \textit{0} & \textit{1} & \textit{2} & \textit{3} & \textit{4} & \textit{5} & \textit{6} & \textit{7} \\ \hline
        \cellcolor[HTML]{C0C0C0}\textbf{Optimizations}   & 4.56       & 11.05      & 26.36      & 54.87      & 112.30     & 250.86     & 1,349.10   & 3,730.33   \\ \hline
        \cellcolor[HTML]{C0C0C0}\textbf{No Optimization} & 40.18      & 99.52      & 243.64     & 514.68     & 1,081.64   & 2,566.25   & 13,157.18  & 37,340.60  \\ \hline
    \end{tabular}
    \caption{Time to create the database with \texttt{wd2duckdb}}
    \label{table:wd2duckdb}
\end{table}

As can be seen, the time it takes \texttt{wd2duckdb} to process the dumps grows at a linear rate. What's more, we can see that the speedup in the execution time is almost 10. Let me first calculate this value, and then, we will comment on the results. Having ${Time_{DEV}}_i$ the $i^{th}$ execution time of the tool in developer mode, that is, with no optimization enabled, and ${Time_{OPT}}_j$ the $j^{th}$ execution time of the tool in release mode, that is, with all the optimizations enabled. The speedup is calculated as follows:

\begin{equation}
    \text{Speedup} = \frac{\overline{Time_{DEV}}}{\overline{Time_{OPT}}} = \frac{\sum_{i=0}^{7}t_i}{\sum_{j=0}^{7}t_j} = \frac{40.18 + \cdots + 37,340.60}{4.56 + \cdots + 3,730.33} = \frac{55,043.70}{5,539.43} = 9.94
\end{equation}

Regarding the size of the database resulting from the execution of the algorithm, it is worth noting that it is hard to compare it to the size of the original dump, as not only the compression rate has an impact on the size of the database, but also the number of columns that we have decided to store, as well as the language chosen for the labels and descriptions. However, we can see that the size of the database is around 10 times smaller than the original dump. To put it into perspective, the size of the original dump is  224.24 GB uncompressed, and 16.94 GB compressed. While the size of the database is 9.38 GB. This means that the database is 23.9 times smaller than the original dump and 1.6 times smaller than the compressed dump. Even though we cannot state that there's a fixed compression rate, we can say that the resulting database is smaller than the original dump. Hence, we can say that the algorithm is efficient in terms of space.

Regarding the complexity of the algorithm, we can say that it is linear. This is because the algorithm iterates over the whole dump, and for each line, it performs a constant number of operations, mainly parsing and inserting a certain Wikidata entity. Hence, the complexity of the algorithm is $O(n)$, where $n$ is the number of lines in the dump. This is also the reason why the time it takes to create the database grows linearly with the size of the dump. This has been proved empirically in the experiment conducted. Refer to Figure \ref{fig:wd2duckdb} for more information.

\section{\texttt{pschema-rs}}

In this section, we will analyze the results obtained from the execution of \texttt{pschema-rs}. Recall that the experiment conducted will be focused on three aspects: the time it takes to create the subsets of Wikidata, the resources used during the execution of the algorithm, and the possibility of creating subsets from other RDF datasets.

The first thing we will analyze is the time it takes to create the subsets of Wikidata. For us to do so, we will execute the algorithm three times, each of them with the same configuration and the same subset of Wikidata, the one of the $21^{st}$ August 2017. Then, we will calculate the average time it takes to create the subsets. Apart from that, we will also track the memory used during the execution of the algorithm. Hence, we will see if it is feasible to create subsets of Wikidata in a machine with acceptable resources. What I mean by this is that we will see if it is possible to create subsets of Wikidata in a machine that, even if it has some resources, is not a supercomputer. Finally, we will compare the results obtained with the ones obtained by \texttt{wdsub}, the tool that we have used as a baseline and that we have described in Section \ref{section:wd2sub}. It is worth noting that this tool is the recommendation of the Wikidata community to create subsets of Wikidata. Hence, we will see if the tool that we have developed is faster than the one recommended by the community.

\begin{table}[ht]
    \centering
    \begin{tabular}{|
            >{\columncolor[HTML]{C0C0C0}}c|c|c|c|c|}
        \hline
        \textbf{N}          & \cellcolor[HTML]{EFEFEF}\textit{0} & \cellcolor[HTML]{EFEFEF}\textit{1} & \cellcolor[HTML]{EFEFEF}\textit{2} & \cellcolor[HTML]{EFEFEF}\textbf{Memory (GB)} \\ \hline
        \textbf{pschema-rs} & 5,014.34                           & 4,995.30                           & 4,996.04                           & 20.3                                         \\ \hline
        \textbf{wdsub}      & 5,090.01                           & 5,064.14                           & 5,071.09                           & -                                            \\ \hline
    \end{tabular}
    \caption{Time to create the \textit{subsets} of Wikidata with \texttt{pschema-rs} and \texttt{wdsub}}
    \label{table:pschema-rs}
\end{table}

As can be seen in table \ref{table:pschema-rs}, both solutions take a similar time to create the subsets of Wikidata. However, we can see that \texttt{pschema-rs} is slightly faster than \texttt{wdsub}. Let me first calculate the speedup, and then, we will comment on the results.

\begin{equation}
    \text{Speedup} = \frac{\overline{Time_{wdsub}}}{\overline{Time_{pschema-rs}}} = \frac{\sum_{i=0}^{2}t_i}{\sum_{j=0}^{2}t_j} = \frac{5,090.01 + \cdots + 5,071.09}{5,014.34 + \cdots + 4,996.04} = \frac{15,225.28}{15,005.69} = 1.01
\end{equation}

As can be seen, the speedup is slightly higher than 1. This means that \texttt{pschema-rs} is slightly faster than \texttt{wdsub}. However, we can see that the difference is not significant. However, far from letting us down, this result is encouraging. Recall that \texttt{wdsub} is a mature tool, while we are still in the early stages of development. Hence, we can say that the results obtained are promising. Moreover, we can see that the memory used by \texttt{pschema-rs} is around 20 GB. This means that it is possible to create subsets of Wikidata in a machine with acceptable resources. Hence, we can say that the algorithm is efficient in terms of time and memory. Having said that, we will now analyze the results obtained from the execution of the algorithm with other RDF datasets.

Before getting into more details, let me define what I mean by \texttt{DEV} and \texttt{OPT}. Recall that in section \ref{section:pschema-rs:optimizations} we showed two main optimizations that we would take advantage from.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|
            >{\columncolor[HTML]{EFEFEF}}c |c|c|c|c|}
        \cline{2-6}
                                                                                     & \cellcolor[HTML]{C0C0C0}\textbf{Shape Expression} & \cellcolor[HTML]{C0C0C0}\textbf{Initial triples} & \cellcolor[HTML]{C0C0C0}\textbf{Resulting triples} & \cellcolor[HTML]{C0C0C0}\textbf{Time (s)} & \cellcolor[HTML]{C0C0C0}\textbf{Memory (GB)} \\ \hline
        \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}}                               & \texttt{protein}                                  & 7,346,129                                        & 226,241                                            & 23.35                                     & 6.74                                         \\ \cline{2-6}
        \multicolumn{1}{|c|}{\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{DEV}}} & \texttt{subcellular\_location}                    & 7,346,129                                        & 1,084,151                                          & 57.56                                     & 6.04                                         \\ \hline
        \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}}                               & \texttt{protein}                                  & 7,346,129                                        & 226,241                                            & 14.58                                     & 3.87                                         \\ \cline{2-6}
        \multicolumn{1}{|c|}{\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{OPT}}} & \texttt{subcellular\_location}                    & 7,346,129                                        & 1,084,151                                          & 37.76                                     & 3.75                                         \\ \hline
    \end{tabular}
    \caption{Time and memory consumption to create the \textit{subsets} of \texttt{Uniprot} with \texttt{pschema-rs}}
    \label{tab:my-table}
\end{table}

\begin{figure}[p]
    \begin{subfigure}{\textwidth}
        \centering
        \includestandalone[width=\textwidth,height=6cm]{diagrams/13-4_pschemaN}
        \caption{Modifying the number of Wikidata Entities}
    \end{subfigure}%
    \vspace*{1em}
    \begin{subfigure}{\textwidth}
        \centering
        \includestandalone[width=\textwidth,height=6cm]{diagrams/13-5_pschemaDEPTH}
        \caption{Modifying the depth of the \texttt{ShEx} tree}
    \end{subfigure}%
    \vspace*{1em}
    \begin{subfigure}{\textwidth}
        \centering
        \includestandalone[width=\textwidth,height=6cm]{diagrams/13-6_pschemaBREADTH}
        \caption{Modifying the breadth of the \texttt{ShEx} tree}
    \end{subfigure}
    \caption{Time to create the \textit{subsets} of Wikidata with \texttt{pschema-rs}}
\end{figure}