\epigraph{\textit{If I have seen further, it is by standing on the shoulders of giants.}}{-- \textup{Isaac Newton}}

Some work has already been done in the field of Knowledge Graph validation. In this section, we are exploring what other projects have achieved and their limitations.

\section{Big data processing and graphs}

Due to the need to handle massive graphs, Google proposed Pregel~\cite{10.1145/1807167.1807184} as a model for large-scale graph computing back in 2010. This idea of \textit{thinking like a vertex} led to the introduction of other systems such as GraphLab~\cite{10.14778/2212351.2212354}, PowerGraph~\cite{180251}, and GraphX~\cite{186216}. The latter is a framework that facilitates the implementation of parallel computing algorithms.

In the present day, alternative approaches have emerged, particularly when considering performance in single-node systems. Rust-based solutions like \texttt{pola-rs}\footnote{\url{https://www.pola.rs/}} have been developed to process large graphs on a single computer due to their highly optimized parallel execution capabilities. This brings up the question of whether it is feasible to process the Wikidata graph on a single computer.

\section{Knowledge graphs}

An introduction to the concept of a Knowledge Graph can be found in~\cite{10.1145/3447772}, which is also referenced in Labra's paper~\cite{https://doi.org/10.48550/arxiv.2110.11709}. In that article, the authors discuss the process of creating a knowledge graph, which bears similarities to the approach used in this document. They also emphasize the significance of data quality, which is the primary focus of this thesis. It is important to note that this document does not deal with the creation of a knowledge graph, but rather focuses on its validation. Furthermore, our work is particularly centered around processing Wikibase graphs, which were introduced in Labra's paper. Our description of Wikibase graphs draws inspiration from MARS (Multi-Attributed Relational Structures)~\cite{ijcai2017p165}, which presents a generalized concept of property graphs. Additionally, Labra's work defines MAPL (Multi-Attributed Predicate Logic) as a formalism of logic that can be applied to ontological reasoning.

\section{Knowledge Graph descriptions}

Several descriptions of knowledge graphs have been proposed, with many introduced in~\cite{https://doi.org/10.48550/arxiv.2110.11709}. Among them, the most relevant ones for this thesis are Property graphs, RDF graphs, and Wikibase graphs. In a Property graph, the nodes represent entities, and the edges represent properties. RDF graphs have entities as nodes and RDF triples as edges. Wikibase graphs, on the other hand, have entities as nodes and Wikibase statements as edges. It is worth noting that the Wikibase graph is a generalization of the RDF graph, and it is the one utilized in this thesis.

To create subsets, we will employ Shape Expressions, which were first introduced in 2014. While the W3C recommendation\footnote{\url{https://www.w3.org/TR/2017/REC-shacl-20170720/}} is SHACL (Shapes Constraint Language) since 2017, the Wikidata community has been using Shape Expressions~\cite{10.1007/978-3-030-21348-0_39}. This preference stems from the fact that Shape Expressions better adapt to describing data models compared to SHACL. A comparison between the two can be found in the following article~\cite{Labra2017}.

Improving the overall quality of knowledge graphs has been the focus of recent research~\cite{https://doi.org/10.48550/arxiv.2110.11709}.

\label{section:wd2sub}
\section{Knowledge Graph subsets}

As previously mentioned, the Wikidata knowledge graph is enormous, and it is not feasible to process it on a single computer using existing techniques. To address this challenge, we have developed a novel method to split the Wikidata graph into smaller subsets using Shape Expressions, as introduced in~\cite{https://doi.org/10.48550/arxiv.2110.11709}.

Although it is possible to create subsets of the RDF Knowledge Graph through SPARQL construct queries, there are limitations to this approach. Notably, the lack of support for recursion, which means representing cyclic data models is not possible. While proposals to extend SPARQL with recursion have been made~\cite{10.1007/978-3-319-25007-6_2}, such extensions are not widely supported by existing processors. Additionally, the verbosity of the solution, which requires the creation of scripts, makes it inconvenient for code development when generating subsets. In light of these limitations, we have developed a new method using Shape Expressions to create subsets of the Wikidata knowledge graph, as described in~\cite{https://doi.org/10.48550/arxiv.2110.11709}.

The creation of subsets in Knowledge graphs has gained attention, starting from the 12th International SWAT4HCLS Conference\footnote{\url{https://www.wikidata.org/wiki/Wikidata:WikiProject_Schemas/Subsetting}}. It has since been selected as a topic of interest in the Elixir Europe Biohackathon 2020\footnote{\url{hhttps://github.com/elixir-europe/BioHackathon-projects-2020/tree/master/projects/35}} and the SWAT4HCLS 2021 hackathon, which resulted in a preprint that collected various proposed approaches~\cite{10.37044/osf.io/wu9et}. The creation of knowledge graph subsets is of interest to the Wikidata community and the field of bioinformatics.

One of the proposed approaches is \texttt{WDumper}\footnote{\url{https://github.com/bennofs/wdumper}}, a tool that processes Wikidata dumps to generate subsets. It takes a JSON compressed dump and a JSON configuration file as inputs, applies filters according to the configuration, and produces an RDF compressed dump. The use of WDumper to generate Wikidata subsets is described in~\cite{wdumper}, where four subsets are created. This paper explores various use-case scenarios and highlights the strengths and weaknesses of this approach. However, one limitation of this approach is its lack of formalism compared to a ShEx-based method.

Another Python library, called WikiDataSets~\cite{boschin2019wikidatasets}, generates Wikidata subsets based on specific topics such as humans, countries, animal species, and films. However, this approach is less flexible compared to the ShEx-based method.

Lastly, \texttt{KGTK} (Knowledge Graph Toolkit)~\cite{ilievski2021kgtk} is a tool that facilitates working with knowledge graphs by introducing a common format called \texttt{KGTK} based on hypergraphs. This allows for the handling of various types of knowledge graphs using the same library. The concept of creating a higher-level abstraction for processing knowledge graphs, as demonstrated in this thesis (Chapter \ref{chapter:pschema}), aligns with the approach of \texttt{KGTK}. In line with \texttt{KGTK}, they have implemented a query language named Kypher~\cite{chalupsky2021creating}, which is an adapted version of \texttt{Cypher} specifically designed for \texttt{KGTK}. This enables the creation of subgraphs within the tool. They utilize SQLite queries to generate subgraphs, and this idea is also adopted in this thesis (see chapter~\ref{chapter:wd2duckdb}). However, we employ \texttt{DuckDB}\footnote{\url{https://duckdb.org/}} as the backend, following a similar approach to the one proposed in \texttt{wd2sql}\footnote{\url{https://github.com/p-e-w/wd2sql}}. The paper referenced above states that they can create subsets of the Wikidata knowledge graph on a laptop. This served as our main inspiration to develop a method for creating subsets on a single machine instead of employing a distributed approach.

Furthermore, during the development of this thesis, we came across a paper that compared several approaches for creating subsets of the Wikidata knowledge graph~\cite{wikidataTools}. In this paper, they evaluated and compared the performance of different approaches and tools. Their methodology for measuring performance and conducting experiments served as the primary inspiration for designing the experiments in chapter~\ref{chapter:experiment}.