\newenvironment{abstract}%
{\cleardoublepage\null\vfill\section*{\abstractname}}%
{\vfill\null}
\begin{abstract}
    The necessity of processing large amounts of data is getting increased with time. Not only we do need to process high volumes of sequential information, but more complex structures such as graphs. One of the sources we can retrieve data from is Wikidata, the central storage for the structured data of Wikimedia projects: including Wikipedia. The shape of the documents to be processed from Wikidata tends to be heterogeneous: the structure of a human node may vary from the one of a mountain. More in more, it could be helpful to generate subsets out of a dump for us to work with concrete nodes.
\end{abstract}

\noindent \textbf{Keywords} --- \textit{Knowledge Graphs, RDF, Linked Data, RDF Validation, Shape Expressions, Subsets, MapReduce, Pregel, Algorithms, Wikibase, Apache Spark, Scala, Rust, DuckDB.}