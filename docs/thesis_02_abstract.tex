\newenvironment{abstract}%
{\cleardoublepage\null\vfill\section*{\abstractname}}%
{\vfill\null}
\begin{abstract}
    The necessity of processing large amounts of data is getting increased with time. Not only we do need to process high volumes of sequential information, but more complex structures such as graphs. One of the sources we can retrieve data from is Wikidata, the central storage for the structured data of Wikimedia projects: including Wikipedia. The shape of the documents to be processed from Wikidata tends to be heterogeneous: the structure of a human node may vary from the one of a mountain. More in more, it could be helpful to generate subsets out of a dump for us to work only with concrete nodes.

    Not only that but, the last tendencies in \textit{data integration} show that the data is not only stored in a single place but distributed among different sources. This is the case of biological data where several different databases are used to store the information, including \texttt{Uniprot} and \texttt{PubChem}. The data is stored in different formats, and it is not always easy to integrate it. However, through \textit{subsets} we can create a common ground for the data to be processed. This is, we can create a subset of the data that is common to all the sources, and then process it.
\end{abstract}

\noindent \textbf{Keywords} --- \textit{Knowledge Graphs, RDF, Linked Data, RDF Validation, Shape Expressions, Subsets, MapReduce, Pregel, Algorithms, Wikibase, Apache Spark, Scala, Rust, DuckDB.}