\epigraph{\textit{It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong.}}{-- \textup{Richard P. Feynman }}

In the following chapter, a description of the theoretical concepts needed for a proper understanding of this document is done. Ranging from \textit{knowledge graphs} to \textit{data-flow algorithms}, we are providing a firm foundation for the experiment to be conducted.

\section{Knowledge graphs}
\label{section:knowledgeGraph}

A knowledge graph uses a graph-structured data model to represent knowledge of some real-world domain. Where each node represents an entity -- a \textit{thing} from the actual world -- and the edges are the relationships between them. This type of graph is often assembled from a wide variety of sources, and as a result, can be highly diverse in terms of structure and granularity~\cite{DBLP:journals/corr/abs-2003-02320}. To address this issue, representations of \textit{schema}, \textit{identity} and \textit{context} are needed. While the former defines the high-level structure of the graph, \textit{identity} relates nodes that conform the same real-world entity; finally, \textit{context} provides the environment for specific knowledge to be understood. These graphs can be modelled using different technologies; however, we will focus on the most representative example of attributed ones: \textit{Wikibase graphs}.

\subsection{Wikibase graphs}

Wikidata started back in 2012 as a support to Wikipedia. Rapidly becoming one of the biggest human knowledge bases, with remarkable organizations donating their data to it; as an example, Google migrated \textit{Freebase} -- its previous knowledge graph -- to Wikidata in 2017~\cite{10.1145/2872427.2874809}. As of October 9, 2022, Wikidata currently contains roughly 100 million items\footnote{\url{https://www.wikidata.org/wiki/Wikidata:Statistics}}. Internally, Wikidata's content is stored in a SQL database (MariaDB); however, this system is not ideal for querying or data analysis. With that in mind, and pursuing the integration of Wikibase within the semantic web ecosystem, the Wikimedia Foundation adopted BlazeGraph: an open-source triple store and graph database\footnote{\url{https://en.wikipedia.org/wiki/Blazegraph}}. This way, two data models coexist in Wikibase: a document-centric model based on MediaWiki, and an RDF-based one, that can be used to perform SPARQL queries through the Query Service~\cite{https://doi.org/10.48550/arxiv.2110.11709}.

\begin{figure}[ht]
    \centering
    \includestandalone{figures/fig_02_architectureWikibase}
    \caption[Simplified architecture of Wikibase]{Simplified architecture of Wikibase~\cite{https://doi.org/10.48550/arxiv.2110.11709}}
    \label{fig:architecture:wikibase}
\end{figure}

Informally speaking, the Wikibase data model is composed by \textit{entities} and \textit{statements} (about those entities). This way, an entity can be either an \textit{item} or a \textit{property}. Items are used to represent all the \textit{things} from the human knowledge. Usually denoted by a \texttt{Q} followed by a sequence of digits; as an example, \href{https://www.wikidata.org/wiki/Q7251}{Q7251} represents Alan Turing in Wikidata. On the other hand, properties model a relationship between an item and a value, and are represented by a \texttt{P} followed by a sequence of numbers; as an example, \href{https://www.wikidata.org/wiki/Property:P31}{P31} is the property \textit{instance of} in Wikidata.

For values to be associated to properties, they must belong to some specific data type\footnote{\url{https://www.wikidata.org/wiki/Help:Data_type}}. Supported data types include: URLs, time, quantities, mathematical expressions; to name a few. Notice how more complex data types are also supported; those include \textit{properties} and \textit{items}, this way, relationships among entities are allowed.

Lastly, a statement is a piece of data about an item, and is recorded in the page of the item itself. Putting it all together, the way Wikibase manages information is as follows: through a statement, we add information to a certain item using some property and its value.

\begin{example}[Adding information to an entity in Wikidata\footnote{\url{https://www.wikidata.org/wiki/Help:Statements}}]
    In order to include information about the genre of \textit{The Hunger Games} in Wikidata, we would need to add a statement to the item itself: \href{https://www.wikidata.org/wiki/Q11679}{The Hunger Games (Q11679)}, using the property \href{https://www.wikidata.org/wiki/Property:P136}{genre (P136)}, we can then add the value \href{https://www.wikidata.org/wiki/Q15062348}{dystopian fiction (Q15062348)}. Notice how we are linking two entities (\textit{items}) of the Wikidata.
\end{example}

\begin{table}[ht]
    \centering
    \input{figures/tab_01_lang}
    \caption[Including information about the genre of \textit{The Hunger Games}]{Including information about the genre of \textit{The Hunger Games}\footnotemark}
    \label{tab:language}
\end{table}
\footnotetext{\url{https://www.wikidata.org/wiki/Q11678}}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{../img/wikidata_statement.png}
    \caption[Example of a simple statement annotating the genre of \textit{The Hunger Games}]{Example of a simple statement annotating the genre of \textit{The Hunger Games}\footnote{\url{https://www.wikidata.org/wiki/Q11678}}}
    \vspace{-3mm}
\end{figure}

\subsubsection{Opaque URIs}

As noted above, the main purpose of Wikidata is to store data about things that are described by pages in Wikipedia (in any language)\footnote{\url{https://www.mediawiki.org/wiki/Wikibase/DataModel}}. This means, \textit{things} should be represented in such a way that they don't depend on the natural language used; that is, language independence is required. In Wikidata, this is achieved through a linked data pattern called \textit{opaque URIs}. In general, a URI is defined as a unique sequence of characters that identifies the resources of the system\footnote{\url{https://en.wikipedia.org/wiki/Uniform_Resource_Identifier}}. Designing good URIs is the first step in a linked data development.

The characters that appear in a URI tend to be related to the concept they represent, improving human-readability. As an example, \texttt{https://example.com/Espa√±a} could -- potentially -- be an identifier for the Spanish country. However, for those non-Spanish speakers, that identifier is meaningless. What's more, the use of non-ASCII characters may lead to problems in an internationalized system. Opposed to \textit{descriptive URIs}, \textit{opaque} ones are not intended to represent terms in a natural language~\cite{LabraGayo2015MultilingualLD}. An example to that is: \url{https://www.wikidata.org/wiki/Q11679}, where item \texttt{Q11679} represents \textit{The Hunger Games} in Wikidata. However -- under some situations -- it is handful for users to interact with human-readable names. In Wikidata, this is solved through \textit{labels}, which are language-dependent. Following the previous example, the label tagged to the entity \texttt{Q11679} is the actual title of the book: \textit{The Hunger Games}. Translations for several languages can also be provided. Notice Table~\ref{tab:language} for an example of possible \textit{labels} for some \textit{opaque URIs}.

What we have seen so far is an overview of the Wikibase \textit{data model}; including: \textit{statements}, \textit{properties}, \textit{values} and the appropriate mechanism for identifying them (in a multilingual approach). Putting it all together, the next step is to formally define a Wikibase graph.

\begin{definition}[Wikibase graph~\cite{https://doi.org/10.48550/arxiv.2110.11709}]
    Given a mutually disjoint set of items \ItemSet{}, a set of properties \PropSet{} and a set of data values \DataValueSet{}, a \emph{Wikibase graph} is a tuple $\langle\ItemSet,\PropSet,\DataValueSet,\StmtSet\rangle$ such that $\StmtSet\subseteq\EntitySet\times\PropSet\times\ValueSet\times\FinSet{\PropSet\times\ValueSet}$ where $\EntitySet=\ItemSet\cup\PropSet$ is the set of entities which can be subjects of a statement and $\ValueSet=\EntitySet\cup\DataValueSet$ is the set of possible values of a property.
\end{definition}

For a better understanding  of this formal definition, let me explain it step-by-step. First, we can add information ($\mathcal{Q}$, $\mathcal{P}$ or $\mathcal{D}$) to any item in $\mathcal{Q}$. Lastly, $\mathcal{V}$ provides extra information to a certain relationship. This is useful when you want to include more data to a particular statement; as an example, in order to establish the \href{https://www.wikidata.org/wiki/Property:P585}{point in time} at which \href{https://www.wikidata.org/wiki/Q937}{Albert Einstein} received the \href{https://www.wikidata.org/wiki/Q38104}{Nobel Prize in Physics}, we may annotate the \href{https://www.wikidata.org/wiki/Property:P166}{award received} property with the value 1921\footnote{\url{https://www.nobelprize.org/prizes/physics/1921/summary/}}. Those are called \textit{qualifiers}. For further clarifications, let us model the Wikibase graph of a certain scenario.

\begin{example}
    \label{example:knowledgeGraph}
    We are willing to qualify that Alan Turing (23 June 1912 -- 7 June 1954) was employed by the government of the United Kingdom in the course of the WWII. During that time he invented a computer for deciphering Enigma-machine-encrypted secret messages\footnote{\url{https://en.wikipedia.org/wiki/Bombe}}. Additional information about relevant places where he lived is also annotated.
\end{example}

\begin{table}[h]
    \vspace{-2mm}
    \centering
    \input{figures/tab_02_wikibaseGraph}
\end{table}

\begin{figure}[H]
    \centering
    \includestandalone[width=0.9\textwidth]{figures/fig_03_wikibaseGraph}
    \caption[Formal description and visualization of Turing's Wikibase graph]{Formal description and visualization of Turing's Wikibase graph}
\end{figure}

\subsubsection{Wikibase data serialization}
\label{section:wikibase:serialization}

The Wikibase data model supports two export formats: JSON and RDF. Being the former the one employed by the JSON dumps and the latter follows semantic web and linked data principles. A fragment of both formats is shown below.

\begin{example}[Seralization of a node using JSON and RDF]
    \href{https://www.wikidata.org/wiki/Q7251}{Alan Turing (Q7251)} was \href{https://www.wikidata.org/wiki/Property:P108}{employed (P108)} by the \href{https://www.wikidata.org/wiki/Q220798}{Government Communications Headquarters (Q220798)} of the United Kingdom, during the World War II (1938-1945).
\end{example}

\paragraph{Wikibase JSON serialization}

The JSON serialization follows the Wikibase data model~\cite{https://doi.org/10.48550/arxiv.2110.11709}. Consisting of a sequence of items where each element is a JSON object capturing all the information about the item itself, represented in a single line. What's remarkable of this mechanism is that it records all the neighboring entities of every single node, making the algorithms validating whole graphs able to process them in a single pass. More on this will be discussed in paragraph \ref{section:pregel}.

\begin{lstlisting}[style=JSON]
[
    { "type": "item", "id": "Q220798", "claims": { "P31": [...
    { "type": "item", "id": "Q7251", "claims": { "P108": [...
    { "type": "property", "id": "P108", "claims": { ... 
    ... 
]
\end{lstlisting}

\paragraph[Wikibase RDF serialization]{Wikibase RDF serialization\footnotemark}
\footnotetext{\url{https://www.wikidata.org/wiki/Special:EntityData/Q7251.ttl}}

The RDF serialization of Wikidata\footnote{\url{https://www.mediawiki.org/wiki/Wikibase/Indexing/RDF_Dump_Format}} is mainly used by the Wikidata's Query Service described in Figure \ref{fig:architecture:wikibase}. The main goal of this formatting is providing support to semantic web technologies. See section \ref{section:RDF} for further details.

\begin{lstlisting}[style=Turtle]
wd:Q7251 p:P108 s:Q7251-5c5c1b1c-40a4-b9c0-40a6-dfaccd56f273 .

s:Q7251-5c5c1b1c-40a4-b9c0-40a6-dfaccd56f273 a wikibase:Statement,
wikibase:BestRank ;
wikibase:rank wikibase:NormalRank ;
ps:P108 wd:Q220798 ;
pq:P580 "1938-01-01T00:00:00Z"^^xsd:dateTime ;
pqv:P580 v:560845b053d80d9ba30cc757808172fd ;
pq:P582 "1945-01-01T00:00:00Z"^^xsd:dateTime ;
pqv:P582 v:db243e11a2e2c6bb034f59e082f66d29 ;
prov:wasDerivedFrom ref:7d58b689209415697990098179e8ace8b04630a1 .
\end{lstlisting}

\section{Knowledge graph validation}

As a brief introduction to this section, let's first describe what validation -- in the context of Knowledge graphs -- is. Wikibase is a collection of data that anyone can edit. This can lead to some concerns regarding the integrity of what is posted in Wikibase, as there is no single point of truth for any given piece of information. According to this, for some piece of data to be considered valid, its description needs to be both syntactically correct and adhere to some type definition. As we have seen in section \ref{section:wikibase:serialization}, the different serialization formats supported by Wikidata include RDF as one of those. Putting it all together, we require a mechanism for ensuring the \textit{validity} of the RDF documents stored in Wikibase.

\subsection{RDF (Resource Description Framework)}
\label{section:RDF}

Resource Description Framework (RDF) is a data interchange W3C standard for the web. It has been used as a method for description and exchange of graph data\footnote{\url{https://en.wikipedia.org/wiki/Resource_Description_Framework}}, providing a variety of syntax notations and formats, with Turtle being the most used one. One of the main features of RDF it that it allows the combination of different data-sources, even if the schemas differ from one another. What's more, it supports the evolution of schemas without requiring the data consumers to adapt to those changes.

RDF is a directed graph composed of triple statements; called \textit{semantic triples}, in the way of \textit{subject-predicate-object}. Where each element is identified by a URI. The \textit{subject} denotes the resource you want to make statements about, the \textit{predicate} denotes attributes of the resource and models relationships among the \textit{subject} and the \textit{object}, and the \textit{object} is the value of the attribute. As an example, we can represent that \textit{the genre of The Hunger Games is dystopian fiction} in RDF as the triple: a \textbf{subject} denoting \textit{The Hunger Games}, a \textbf{predicate} denoting \textit{is of genre}, and an \textbf{object} denoting \textit{dystopian fiction}. That leads us to the following graph representation:

\begin{figure}[ht]
    \centering
    \includestandalone{figures/fig_04_RDF}
    \caption{Directed knowledge graph generated out of a triple \textit{subject-predicate-object}}
    \label{fig:RDF}
\end{figure}

We have mentioned that the most common syntax notation for RDF is Turtle; however, one of the simplest is N-Triples. Each line of a \texttt{.nt} document is a single triple \textit{subject-predicate-object} that together form a directed knowledge graph. Let's see an example of an RDF graph represented using N-Triples notation.

\begin{example}[RDF knowledge graph using N-Triples notation]
    \href{https://www.wikidata.org/entity/Q7251}{Alan Turing (Q7251)} \href{https://www.wikidata.org/entity/P19}{was born (P19)} in \href{https://www.wikidata.org/entity/Q20895942}{Warrington Lodge (Q20895942)} and \href{https://www.wikidataWShEx.org/entity/P108}{was employed (P108)} by the \href{https://www.wikidata.org/entity/Q220798}{Government of the United Kingdom (Q220798)} where he \href{https://www.wikidata.org/entity/P61}{discovered (P61)} the Enigma-deciphering machine \href{https://www.wikidata.org/entity/Q480476}{bombe (Q480476)}.
\end{example}

\begin{lstlisting}[style=NTriples]
# Notice how http://wikidata.org/wiki/ID is written as http://wikidata.org/ID for short
<http://wikidata.org/Q7251> <http://wikidata.org/P19>  <http://wikidata.org/Q20895942> .
<http://wikidata.org/Q7251> <http://wikidata.org/P108> <http://wikidata.org/Q220798> .
<http://wikidata.org/Q220798> <http://wikidata.org/P176> <http://wikidata.org/Q480476> .
<http://wikidata.org/Q480476> <http://wikidata.org/P61>  <http://wikidata.org/Q7251> .
\end{lstlisting}

\subsection{ShEx (Shape Expressions)}

Shape Expressions (ShEx) were designed as a high-level, domain-specific language for describing RDF graph structures~\cite{https://doi.org/10.48550/arxiv.2110.11709}. The syntax of ShEx is inspired by Turtle and SPARQL, while the semantics is inspired by RelaxNG and XML Schema.

\begin{lstlisting}[style=Turtle]
:Person {
    :birthPlace @:Place ;
    :birthDate @:Date ;
    :employer @:Organization ;
}
:Place {
    :country @:Country
}
:Country {}
:Organization {}
:Date {} 
\end{lstlisting}

\subsubsection{EntitySchemas}

Back in 2019, Wikibase adopted ShEx as the language to define entity schemas\footnote{\url{https://www.wikidata.org/wiki/Wikidata:Database_reports/EntitySchema_directory}}. However, instead of describing the Wikibase data model, they describe the RDF serialization of Wikibase entities. As we have seen in section \ref{section:wikibase:serialization}, RDF serialization is just a copy -- in RDF -- of the JSON dump, and we cannot use ShEx for JSON validation. This means, the solution provided using ShEx is further away from the Wikibase data model.

\begin{lstlisting}[style=Turtle]
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX wd: <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>

start = @<human>

<human> EXTRA wdt:P31 {
    wdt:P31 [wd:Q5];
    wdt:P18 . * ;               # image (portrait)
    wdt:P21 [wd:Q48270 wd:Q48279 wd:Q179294 wd:Q189125 wd:Q207959 wd:Q301702 wd:Q350374 wd:Q505371 wd:Q660882 wd:Q746411 wd:Q859614 wd:Q1052281 wd:Q1097630 wd:Q1289754 wd:Q1399232 wd:Q2449503 wd:Q3177577 wd:Q3277905 wd:Q6581072 wd:Q6581097 wd:Q7130936 wd:Q12964198 wd:Q15145778 wd:Q15145779 wd:Q18116794 wd:Q27679684 wd:Q27679766 wd:Q52261234 wd:Q93954933 wd:Q93955709 wd:Q96000630 wd:Q25388691 wd:Q56315990]?;   # gender
    wdt:P19 . ?;                      # place of birth
    wdt:P20 . ?;                      # place of death
    wdt:P569 . ? ;                    # date of birth
    wdt:P570 . ? ;                   # date of death
    wdt:P735 . * ;                    # given name
    wdt:P734 . * ;                    # family name
    wdt:P106 . * ;                    # occupation
    wdt:P1559 . ? ;              #name in native language
    wdt:P27 @<country> *;           # country of citizenship
    wdt:P22 @<human> *;             # father
    wdt:P25 @<human> *;             # mother
    wdt:P3373 @<human> *;           # sibling
    wdt:P26 @<human> *;             # spouse
    wdt:P40 @<human> *;             # children
    wdt:P1038 @<human> *;           # relatives
    wdt:P103 @<language> *;         # native language
    wdt:P1412 @<language> *;        # languages spoken, written or signed
    wdt:P6886  @<language> *;       # writing language
    rdfs:label rdf:langString+;
}

<country> EXTRA wdt:P31 {
    wdt:P31 [wd:Q6256 wd:Q3024240 wd:Q3624078] +;
}

<language> EXTRA wdt:P31 {
    wdt:P31 [wd:Q34770 wd:Q1288568] +;
}  
\end{lstlisting}

\subsubsection{WShEx}

WShEx is a language inspired by ShEx that provides a support for the Wikibase data model and can be used to describe and validate Wikibase entities~\cite{https://doi.org/10.48550/arxiv.2208.02697}. What's more, the main motivation for developing this language was to create subsets of Wikidata using a human-readable language.

As a solution to the problem described before, an extension of ShEx for describing the Wikibase data model was designed. Figure \ref{fig:WShEx} represents the relationships between ShEx, WShEx and the Wikibase data model.

\begin{figure}[ht]
    \centering
    \includestandalone{figures/fig_05_WShEx}
    \caption[Relationships between ShEx, WShEx and the Wikibase data model]{Relationships between ShEx, WShEx and the Wikibase data model~\cite{https://doi.org/10.48550/arxiv.2110.11709}}
    \label{fig:WShEx}
\end{figure}

\begin{lstlisting}[style=Turtle]
:Human {
    birthPlace @ < Place > ;
    birthDate @ < Time > ;
    employer @ < Organization > *
        {{ :start @:Date ,
        :end @:Date
        }} ;
    awarded @ < Award > *
        {{ :pointTime @:Date ,
        :togetherWith @:Person
        }}
}
:Place { country @ < Country > }
:Organization {}
:Award { country @ < Country > }
:Country {}
:Date xsd:date
\end{lstlisting}

\subsection{Validating Wikibase graphs}

In the previous paragraphs we have discussed different technologies related to Knowledge graph validation; however, as we have stated in section \ref{section:knowledgeGraph}, we are aiming for a solution to validate Wikibase Graphs.

\section{Knowledge graph Subsets}

\section{Data-flow algorithms}

The main focus of this document is implementing a Big data solution using the Pregel algorithm. However, for better understanding it, introducing the MapReduce model will be handful. Notice that both are meant to be executed in parallel in a distributed system.

\subsection{MapReduce}

Inspired by the map and reduce functions, a MapReduce~\cite{wiki:MapReduce} program is composed of a \textit{map procedure}, where we apply a simple operation to all the elements of a sequence, followed by a \textit{reduce} method, which transforms those elements into a single result. For us to process a graph, we would need to chain MapReduce invocations. Where, for each iteration, map and reduce functions are applied. The main drawback of this approach, is the functional nature of the MapReduce model. This means, expressing a graph algorithm as a chained MapReduce results in having to pass the entire state of the graph from one stage to another~\cite{10.1145/1807167.1807184}.

\subsection{Pregel model}
\label{section:pregel}

Pregel (\textit{Parallel, Graph and Google}) is a data flow paradigm and system created by Google to handle large-scale graphs. Even though the original system remains proprietary at Google, the computational model was adopted by many graph-processing systems: including Apache Spark. For better understanding Pregel, the idea is to \textit{think like a vertex}; this way, for computing the state of a given node, we only depend on the states of its neighboring ones. We will call neighboring vertices of a certain one to those nodes connected to it by an outgoing edge. This has a relevance for better understanding the architecture of this system depicted in Figure \ref{fig:architecture:pregel}. \textit{Thinking like a vertex} could be understood as the \textit{leitmotif} for dividing the problem into several sub-problems: instead of dealing with a huge graph, we just have to solve the problem for smaller graphs: a vertex and its neighboring ones. Notice how a graph to be processed with Pregel may potentially have millions of vertices with billions of edges. More on the size of the Wikibase graphs will be discussed later on.

In comparison to the MapReduce framework, where for each iteration the state of the whole graph must be passed, at each \textit{superstep} -- the way we refer to iterations in Pregel -- each vertex can: send a message to its neighbors, process the received messages (from the previous superstep), and update its state. Summing up, instead of sending the whole state of the graph, we just send messages back and forth. The best way for understanding this is through an example. Let me show you the resulting trace after applying Pregel for computing the maximum value in a graph (see figure \ref{fig:pregel}).

\begin{figure}[ht]
    \centering
    \includestandalone{figures/fig_06_pregelTrace}
    \caption[Trace of the execution of Pregel for computing the maximum value]{Trace of the execution of Pregel for computing the maximum value~\cite{10.1145/1807167.1807184}}
    \label{fig:pregel}
\end{figure}

Notice that at the beginning of the execution, the state of all the nodes will be set to active. This initial stage is called \textit{superstep 0}. When a vertex is active, it sends a message to its neighbors, that will receive it in the next \textit{superstep}. In this case, the message we are propagating is the largest value that we have learned so far. As an example to that, the second to the left node starts with a value of 6 -- which is the maximum value of the graph -- and sends that number to its neighbors: 3 and 1. In the next \textit{superstep}, the vertices that have received a message have to compare both: the value they store and the received one. This comparison is what it's called the \textit{vProg} function, which will vary from one problem to another. Then, both nodes will update its state to 6. As they have updated their state, they will have to send the new value to its neighbors: beginning another iteration (\textit{superstep}). Notice that at the \textit{superstep 1}, the second to the left node is halted as it doesn't send any other message to its neighbors as its value has not changed. When every node is halted -- inactive -- the execution of the algorithm finishes. In Figure \ref{fig:state} a simplified state machine is shown.

\begin{figure}[ht]
    \centering
    \includestandalone{figures/fig_07_stateDiagram}
    \caption[Simplified state diagram of a vertex]{Simplified state diagram of a vertex~\cite{10.1145/1807167.1807184}}
    \label{fig:state}
\end{figure}

\subsubsection{Architecture of a Pregel system}

It's quite simple to describe the architecture of a Pregel system. As we have seen, one of the main goals of this algorithm is achieving a parallel execution. This way, a \textit{master} node will divide the graph into several partitions and assign one (or more) of them to each \textit{worker} node. For the \textit{master} to create the partitions it selects a bunch of vertices and all those vertices' outgoing edges; remember: \textit{think like a vertex}.

\begin{figure}[ht]
    \centering
    \includestandalone[width=0.75\textwidth]{figures/fig_08_pregelArchitecture}
    \caption[Architecture of a Pregel system]{Architecture of a Pregel system~\cite{10.1145/3349265}}
    \label{fig:architecture:pregel}
\end{figure}