\epigraph{\textit{The programmers of tomorrow are the wizards of the future. You're going to look like you have magic powers compared to everybody else.}}{-- \textup{Gabe Newell}}

Having the results obtained during the development of this project, it is possible to draw some conclusions about the work done. In this chapter, I will present them, as well as some achievements and future work that can be done to improve the tools presented.

First, I want to acknowledge that one of the main objectives of this project was to learn about the technologies used to develop it. Ranging from the programming language used, Rust, to other technologies such as those from the Semantic Web stack, or other more abstract concepts such as new data structures, optimization techniques and graph algorithms. In this regard, I can say that I have achieved this objective and I am very proud of the work done. What's more, I have also learned other soft skills such as how a research project is conducted, including how to write a scientific paper, and how to present it. Lastly, I have also learned how to manage an open-source project.

Regarding the tools presented in this project, I think that they introduce some interesting novel approaches to the problem of creating subsets of a Knowledge Graph. Namely, the use of \textit{Rust} as a programming language, and the idea of a \textit{multi-threaded} Pregel implementation. However, I think that the tools can be improved in many ways and that they are still in the early stages of development.

The results of this project are three different tools serving several purposes. The first one is a tool that can be used for processing JSON dumps of \textit{Wikidata} and converting them into \texttt{DuckDB} databases. This pursues the goal of making the data both easier to work with and to share with others, as the resulting database is much smaller than the original JSON dump. This also explores the idea of working with \textit{columnar} databases and trying to store data in a more structured manner. The second is a Pregel implementation that can be used to write graph algorithms simply. Not only that, but it follows the idea of a multi-threaded Pregel, instead of the traditional distributed fashion. This is because the tool is intended to be used in a single machine, and not in a cluster of them, but taking advantage of the multiple cores of nowadays processors. Lastly, the third one is a tool that can be used to create subsets of a Knowledge Graph. This tool is intended to be used with the \textit{Wikidata} database, but it can be used with any other Knowledge Graph, as long as it is in \texttt{DuckDB} or \texttt{N-Triples} format. This tool is written using the Pregel implementation presented above and follows a \textit{reverse level-order}-based traversal to create the subsets.

Lastly, we have also conducted several experiments for testing the limits of the tools and to check their performance compared to other competitors. In this regard, I think that the results obtained are better than expected, as the \texttt{pschema-rs} tool can process Knowledge graphs with limited resources and in a reasonable amount of time. What's more, we can see that the tool excels at creating subsets of smaller Knowledge Graphs, but it can also be used to create subsets of larger ones, as long as the machine has enough resources.

\section{Achievements}

From June the $25^{th}$ to July the $1^{st}$, the \textit{DBCLS BioHackathon 2023} took place, this is an event focused on the standardization and interoperability of life sciences and biomedical databases, namely, Knowledge Graphs. Following this, during the event, the \texttt{pschema-rs} tool was presented and it was used to create some subsets of the \textit{Uniprot} database. Which was later published in the \textit{Zenodo} repository \cite{angel_iglesias_prestamo_2023_8086938}. This is a great achievement for the tool, as it was used in a real-world scenario, and it was also used by other people, which is one of the main goals of the project. Moreover, this served as my first-ever publication in the scientific community, which is something that I am very proud of. Not only the \textit{dataset} was published, but also a pre-print of a paper that was submitted to the \textit{BioHackrxiv} repository \cite{labra-gayo_waagmeester_yamamoto_iglesias-pr√©stamo_katayama_liener_unni_bolleman_aoki-kinoshita_yokochi_et}. This paper was written by the team that worked on the \textit{data integration} project, and it was also my first-ever scientific paper.

\section{Future work}

The \texttt{pschema-rs} tool presented in this project is still in its early stages, and many improvements can be made to it. Some of the most important ones are:

\begin{enumerate}
    \itemsep0.5em
    \item \textbf{Improve the performance of the tool}: the tool is written on top of the \texttt{pola-rs} library, which is still in its early stages. This means that many improvements can be made to the library, and that will be reflected in the tool. What's more, the tool can be improved by exploring other compiler configurations and optimization techniques.
    \item \textbf{Add more features}: as we have seen in the previous chapters, the tool was implemented as a proof of concept, and it lacks many features that would be useful. This is, currently, only part of the \texttt{ShEx} language is supported, and the tool can only be used to create subsets considering certain constraints, but not the whole specification. What's more, the tool can be improved by adding support to other \textit{backends} such as \texttt{PostgreSQL} or \texttt{MySQL}, or other \textit{RDF} formats such as \texttt{RDF/XML} or \texttt{Turtle}.
    \item \textbf{Improve the documentation}: the documentation of the tool can be improved by adding more examples and use cases, as well as improving the documentation of the code itself.
    \item \textbf{Writing some Python bindings}: the tool is written in \textit{Rust}, which is a language that is not widely used in the scientific community. This means that the tool is not accessible to many people, as they would have to learn a new programming language to use it. To solve this, some Python bindings could be written.
    \item \textbf{Create a web interface}: the tool can be improved by creating a web interface that can be used to create subsets of Knowledge Graphs. This would make the tool more accessible to people that do not have any programming knowledge.
\end{enumerate}

\section{Final words}

To conclude, I want to say that I am very proud of the work done not only during the development of this project but also during the whole degree. I have learned a lot of things, and I have met a lot of people that have helped me to grow as a person. I am very grateful to all of them, and I hope that I can continue to learn from them in the future. I am also grateful for the opportunity this project gave to me as it served as an inspiration to pursue a research career. I hope that I can continue to work on this project in the future and that I can continue to contribute to the scientific community. The future is bright, and I am looking forward to it.